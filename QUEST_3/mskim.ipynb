{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "평가문항\n",
    "    - 상세기준\n",
    "1. 케글 데이터 분석 전과정이 성공적으로 진행되었는가?\n",
    "    - 작성한 노트북을 케글에 제출했다.\n",
    "2. 전처리, 학습과정 및 결과에 대한 설명이 시각화를 포함하여 체계적으로 진행되었는가?\n",
    "    - 제출한 주피터노트북 파일이 케글 커널 환경에서도 에러 없이 동작하며, 전처리, 학습, 최적화 진행 과정이 체계적으로 기술되었다.\n",
    "3. 회귀모델 예츠 정확도가 기준 이상 높게 나왔는가?\n",
    "    - 다양한 피처 엔지니어링과 하이퍼 파라미터 튜닝 등의 최적화 기법을 통해 케클 리더보드의 Private score 기준 110000 이하의 점수를 보였다.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 특이사항\n",
    "\n",
    "로컬(MAC)에서 테스트르르 하다가 import error발생 (xgboost)\n",
    "anaconda activation 중이었긴했지만 pip install --upgrade xgboost를 수행하는 바람에 제대로 된 셀 별 작업이 수행되지 않음\n",
    "\n",
    "이에 노드(4-2)에서 작업한 코드를 하나의 셀에 작성하며 결과는 캡쳐본을 마크다운으로 명시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 그래프 준비\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# 필요한 라이브러리 임포트 하기\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 내 입맛대로 데이터 준비하기\n",
    "# 노드 예시\n",
    "data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\n",
    "# 로컬 예시\n",
    "# data_dir = '../data/kaggles/kakr_housing'\n",
    "\n",
    "train_data_path = join(data_dir, 'train.csv')\n",
    "test_data_path = join(data_dir, 'test.csv') \n",
    "\n",
    "## train, test에 kaggle data load done\n",
    "train = pd.read_csv(train_data_path)\n",
    "test = pd.read_csv(test_data_path)\n",
    "\n",
    "# 데이터 확인\n",
    "train.head()\n",
    "\n",
    "# datetime 컬럼의 값들을 6자리 정수로 변환하고 데이터 확인\n",
    "train['date'] = train['date'].apply(lambda i: i[:6]).astype(int)\n",
    "train.head()\n",
    "\n",
    "# 학습데이터(train)에서 가격('price')를 빼기 위해 작업; 컬럼 삭제\n",
    "# 가격은 변수 y에 저장함\n",
    "y = train['price']\n",
    "del train['price']\n",
    "\n",
    "print(train.columns)\n",
    "\n",
    "# 학습데이터(train)에서 id를 삭제하고 컬럼 확인\n",
    "del train['id']\n",
    "\n",
    "print(train.columns)\n",
    "\n",
    "# test 데이터에서 데이터 전처리\n",
    "## price 컬럼은 없으니 처리 생략하기\n",
    "# [[YOUR CODE]]\n",
    "# 데이터 살펴보기\n",
    "test.head()\n",
    "# 테스트 데이터의 'date' 컬럼 정수형으로 변경\n",
    "test['date'] = test['date'].apply(lambda i: i[:6]).astype(int)\n",
    "test.head()\n",
    "\n",
    "# price 컬럼 작업 생략\n",
    "\n",
    "# id 컬럼 삭제 및 나머지 컬럼 확인\n",
    "del test['id']\n",
    "\n",
    "print(test.columns)\n",
    "\n",
    "# 타켓 데이터(y) 확인\n",
    "y\n",
    "\n",
    "# 데이터 분포도를 시각화하여 확인\n",
    "\"\"\"\n",
    "seaborn의 `kdeplot`을 활용해 `y`의 분포를 확인해주세요!\n",
    "\"\"\"\n",
    "\n",
    "#코드 작성\n",
    "sns.kdeplot(y, shade=True)\n",
    "plt.show()\n",
    "\n",
    "# 좌편향 데이터를 로그 변환\n",
    "y = np.log1p(y)\n",
    "y\n",
    "\n",
    "# 로그화된 데이터 분포를 시각화\n",
    "sns.kdeplot(y)\n",
    "plt.show()\n",
    "\n",
    "# 전체 학습데이터 자료형 확인\n",
    "train.info()\n",
    "\n",
    "# 훈련 데이터셋과 검증 데이터셋으로 나눌 train_test_split() 호출\n",
    "# RMSE 계산을 위한 모듈 호출\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# RMSE를 구하는 함수 선언\n",
    "def rmse(y_test, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))\n",
    "\n",
    "\n",
    "# 네 가지 모델 가져오기\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# 모델 인스턴스 생성 및 결집\n",
    "# random_state는 모델초기화나 데이터셋 구성에 사용되는 랜덤 시드값입니다. \n",
    "#random_state=None    # 이게 초기값입니다. 아무것도 지정하지 않고 None을 넘겨주면 모델 내부에서 임의로 선택합니다.  \n",
    "random_state=2021        # 고정값을 2021로 변경해봄\n",
    "\n",
    "gboost = GradientBoostingRegressor(random_state=random_state)\n",
    "xgboost = XGBRegressor(random_state=random_state)\n",
    "lightgbm = LGBMRegressor(random_state=random_state)\n",
    "rdforest = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "models = [gboost, xgboost, lightgbm, rdforest]\n",
    "\n",
    "# 모델 이름 확인; __class__.__name__을 통해 가능\n",
    "gboost.__class__.__name__\n",
    "\n",
    "# 모델 별 학습 및 예측을 함수화\n",
    "def get_scores(models, train, y):\n",
    "    # 답안 작성\n",
    "    df = {}\n",
    "\n",
    "    for model in models:\n",
    "        # 모델 이름 획득\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "        # train, test 데이터셋 분리\n",
    "        # random_state를 사용하여 고정하고 train과 test 셋의 비율은 8:2로 합니다.\n",
    "        # [[YOUR CODE]]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "        # 모델 학습\n",
    "        # [[YOUR CODE]]\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # 예측\n",
    "        # [[YOUR CODE]]\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        # 예측 결과의 rmse값 저장\n",
    "        df[model_name] = rmse(y_test, y_pred) # [[YOUR CODE]]\n",
    "        \n",
    "        # data frame에 저장\n",
    "        score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)\n",
    "\n",
    "    return df\n",
    "# 모든 모델에서 score 획득해보기\n",
    "get_scores(models, train, y)\n",
    "\n",
    "# 다양한 하이퍼 파라미터 튜닝 테스트\n",
    "## GridSearchCV 클래스 활용\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# xgboost 관련 하이퍼 파라미터 준비\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [1, 10],\n",
    "}\n",
    "\n",
    "# GridSearch할 함수 선언\n",
    "def my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5):\n",
    "    # GridSearchCV 모델로 초기화\n",
    "    grid_model = GridSearchCV(model, param_grid=param_grid, scoring='neg_mean_squared_error', \\\n",
    "                              cv=5, verbose=verbose, n_jobs=n_jobs)\n",
    "    # 모델 fitting\n",
    "    grid_model.fit(train, y)\n",
    "\n",
    "    # 결과값 저장\n",
    "    params = grid_model.cv_results_['params']\n",
    "    score = grid_model.cv_results_['mean_test_score']\n",
    "    \n",
    "    # 데이터 프레임 생성\n",
    "    results = pd.DataFrame(params)\n",
    "    results['score'] = score\n",
    "    \n",
    "    # RMSLE 값 계산 후 정렬\n",
    "    results['RMSLE'] = np.sqrt(-1 * results['score'])\n",
    "    results = results.sort_values('RMSLE')\n",
    "\n",
    "    return results\n",
    "\n",
    "# GridSearch 성능확인\n",
    "model = LGBMRegressor(random_state=random_state)\n",
    "# n_jobs는 CPU 사용개수; 병렬처리;\n",
    "results = my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5)\n",
    "\n",
    "\n",
    "\n",
    "# sort_values()를 사용하여 RMSLE가 낮은 순서대로 정렬하기\n",
    "# 위의 표를 `RMSLE`가 낮은 순서대로 정렬해주세요.\n",
    "results.sort_values(by=['RMSLE'])\n",
    "\n",
    "# 가장 좋은 조합인 max_depth=10, n_estimators=100 가지고 모델 학습 수행; 노드 기준\n",
    "# 테스트는 8, 120으로 해봄\n",
    "# 이후 예측값을 submission.csv 파일로 생성 후 kaggle에 제출\n",
    "model = LGBMRegressor(max_depth=8, n_estimators=240, random_state=random_state)\n",
    "model.fit(train, y)\n",
    "prediction = model.predict(test)\n",
    "prediction\n",
    "\n",
    "## 로그변환값(RMSLE)을 원래 스케일로 되돌리기\n",
    "prediction = np.expm1(prediction)\n",
    "prediction\n",
    "\n",
    "# sample_submission.csv 파일 가져오기\n",
    "# 노드 예시 경로\n",
    "data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\n",
    "# data_dir = '../data/kaggles/kakr_housing'\n",
    "\n",
    "submission_path = join(data_dir, 'sample_submission.csv')\n",
    "submission = pd.read_csv(submission_path)\n",
    "submission.head()\n",
    "\n",
    "# sample dataframe의 price에 예측값 덮어씌우기\n",
    "# kaggle 제출용 데이터 만들어짐\n",
    "submission['price'] = prediction\n",
    "submission.head()\n",
    "\n",
    "# 위 데이터를 CSV 파일로 저장\n",
    "# 모델 종류와 테스트일시를 파일에 명시\n",
    "submission_csv_path = '{}/submission_{}_RMSLE_{}.csv'.format(data_dir, 'lgbm', '20230712T0954')\n",
    "submission.to_csv(submission_csv_path, index=False)\n",
    "print(submission_csv_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![결과화면1](./images/1.png \"결과화면1\")</br>\n",
    "![결과화면2](./images/2.png \"결과화면2\")</br>\n",
    "![결과화면3](./images/3.png \"결과화면3\")</br>\n",
    "![결과화면4](./images/4.png \"결과화면4\")</br>\n",
    "</br>\n",
    "\n",
    "csv는 다음과 같다.</br>\n",
    "```text\n",
    "    1. submission_lgbm_RMSLE_20230712.csv\n",
    "        - random_state = 2021, max_depth = 10, n_estimaters = 100\n",
    "    2. submission_lgbm_RMSLE_20230712T0741.csv\n",
    "        - random_state = 2021, max_depth = 8, n_estimaters = 120\n",
    "    3. submission_lgbm_RMSLE_20230712T0954.csv\n",
    "        - random_state = 2021, max_depth = 8, n_estimaters = 240\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
